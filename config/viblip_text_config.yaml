data:
  images_train_folder: data/st_images
  train_dataset: data/train.json
  images_val_folder: data/st_images
  val_dataset: data/dev.json
  test_dataset: data/test.json
  num_worker: 0

tokenizer:
  padding: max_length
  max_input_length: 300 #if use caption set 300, if not use set 40
  max_target_length: 32
  truncation: True
  return_token_type_ids: True
  return_attention_mask: True

text_embedding:
  text_encoder: VietAI/vit5-base
  remove_accents_rate: 0
  use_caption: True
  freeze: False
  use_lora: False

qformer_embedding:
  qformer_encoder: vinai/phobert-base
  num_query_tokens: 64
  freeze: True

vision_embedding:
  image_encoder: google/vit-base-patch16-224-in21k
  freeze: True
  already_extracted: True
  feature_path: data/feature_VIT
  max_seq: null #if not use, set 0

ocr_embedding:
  sort_type: top-left bottom-right # score or top-left bottom-right
  path_ocr: data/swintext_spotter
  threshold: 0.3
  remove_accents_rate: 0
  max_scene_text: 32  ##if not use, set 0
  d_det: 256
  d_rec: 256
  max_2d_position_embeddings: 1024
  num_distances: 32
  d_feature: 768

generator_args:
  max_length: 32
  min_length: 1
  num_beams: 4
  length_penalty: 1.5
  no_repeat_ngram_size: 3
  early_stopping: True

model:
  type_model: viblip_text  # Custom name for the multimodal model

train:
  output_dir: checkpoint
  cuda_device: cuda:0
  precision: float16
  seed: 12345
  num_train_epochs: 10
  patience: 3
  learning_rate: 2.0e-4
  weight_decay: 0.0
  metric_for_best_model: em
  per_device_train_batch_size: 32
  per_device_valid_batch_size: 32

infer:
  with_answer: True
  images_test_folder: data/st_images
  test_dataset: data/test.json
  per_device_eval_batch_size: 32